#!/usr/bin/python
#	Note : This is designed for Python 3


from optimization import *
import numpy as np

class orthogonal_optimization(optimization):
	def __init__(self, db):
		self.cost_function = db['compute_cost']
		self.gradient_function = db['compute_gradient']
		self.x_opt = None
		self.cost_opt = None
		self.db = db

		#self.db = {}
		#self.db['run_debug_1'] = True

	def calc_A(self, x):
		G = self.gradient_function(x)
		A = G.dot(x.T) - x.dot(G.T)
		return A

	def compute_gradient(self, x):
		A = self.calc_A(x)
		return A.dot(x)

	def run(self, x_init, max_rep=400):
		d = x_init.shape[0]
		self.x_opt = x_init
		I = np.eye(d)
		converged = False
		x_change = np.linalg.norm(x_init)
		m = 0
		while( (converged == False) and (m < max_rep)):
			alpha = 2
			cost_1 = self.cost_function(self.x_opt)
			A = self.calc_A(self.x_opt)

			while(alpha > 0.000000001):
				next_x = np.linalg.inv(I + alpha*A).dot(I - alpha*A).dot(self.x_opt)
				cost_2 = self.cost_function(next_x)
				#print(alpha)
	
				if 'run_debug_1' in self.db: print(alpha, cost_1, cost_2)
				if((cost_2 < cost_1) or (abs(cost_1 - cost_2)/abs(cost_1) < 0.0000001)):
					x_change = np.linalg.norm(next_x - self.x_opt)
					[self.x_opt,R] = np.linalg.qr(next_x)		# QR ensures orthogonality
					self.cost_opt = cost_2
					break
				else:
					alpha = alpha*0.2

			m += 1

			if 'run_debug_2' in self.db: print('Cost Norm : %.3f'%cost_2)
			if 'run_debug_3' in self.db: print('Gradient Norm : %.3f'%np.linalg.norm(self.compute_gradient(self.x_opt)))

			#print(x_change)
			if(x_change < 0.001*np.linalg.norm(self.x_opt)): converged = True

		return self.x_opt	
